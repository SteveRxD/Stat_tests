[
["three-or-more-means.html", "Chapter 7 Three or more means 7.1 One-way ANOVA 7.2 Two-way ANOVA 7.3 ANCOVA", " Chapter 7 Three or more means 7.1 One-way ANOVA The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups. Examples include: Is there a difference in academic outcomes for pupils from ten different schools? Is there a difference daily coffee consumption between people in three different countries? Intuitively, ANOVA is based on comparing the variance (or variation) between the groups, to variation within each particular group. If the ‘between’ variation is much larger than the ‘within’ variation, we are more likely to conclude that the means of the different samples are not equal. If the ‘between’ and ‘within’ variations are more similar in size, then we are less likely to conclude that there is a significant difference between sample means. Why can’t we just compare the means of every possible pair of groups, and see if any differences are statistically significant? The reason is that as the number of groups increases, the more likely we are to see differences that are due to chance alone. This means we are more likely to commit a Type I error, rejecting the null hypothesis (that there is no difference between the means) when the null hypothesis is in fact true.1 An ANOVA controls for this additional risk of Type I errors. Dataset: To illustrate, we will create a new dataset (mydata_anova1). We assume three groups (A, B and C) of normally-distributed variables, with means of 0, 1 and 0.5 respectively. We also create indicator/dummy variables for groups B and C (group A is our reference group, and so does not require an indicator): # Create dataset &#39;mydata_anova1&#39; which is three groups: N &lt;- 20 mydata_anova1 &lt;- data.frame( value = c(rnorm_fixed(N,mu = 0, sd = 1), # Group A rnorm_fixed(N, mu = 1, sd = 1), # Group B rnorm_fixed(N, mu = 0.5, sd = 1)), # Group C group = rep(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), each=N) ) %&gt;% # Explicitly add indicator/dummy variables mutate(group_b = if_else(group == &#39;b&#39;, 1 , 0)) %&gt;% # Group B dummy mutate(group_c = if_else(group == &#39;c&#39;, 1 , 0)) # Group C dummy Here’s a random sample of rows from our new data set, which inclues the dummy variables: Table 7.1: Some randomly selected rows from our data set value group group_b group_c -1.2447366 a 0 0 -0.4855228 a 0 0 1.6896629 b 1 0 -1.3609026 c 0 1 1.1446378 c 0 1 -0.0555862 c 0 1 Built-in ANOVA function: R has a built-in function for ANOVA analysis in this case car::Anova(aov()). However, this is simply a ‘wrapper’ around the equivalent linear model, described below, and yields identical results. Equivalent linear model: The linear model assumes that the dependent variable can be predicted with a single mean for each group: \\(y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + ... \\qquad H_0: y = \\beta_0\\) This assumption is illustrated below. This extends the case with two groups (the independent-sample t-test, illustrated in figure ??) to a case with three or more groups. The linear model makes the assumption that neither \\(\\beta_1\\) nor \\(\\beta_2\\) are signficantly different from zero; or equivalently, that all groups have the same mean of \\(\\beta_0\\). Figure 7.1: Linear model equivalent to ANOVA with three groups Comparison: car::Anova(aov(value ~ group, data = mydata_anova1)) ## Anova Table (Type II tests) ## ## Response: value ## Sum Sq Df F value Pr(&gt;F) ## group 10 2 5 0.009984 ** ## Residuals 57 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Linear model lm &lt;- lm(value ~ 1 + group_b + group_c, data = mydata_anova1) lm %&gt;% summary() %&gt;% print(digits = 8) # show summary output ## ## Call: ## lm(formula = value ~ 1 + group_b + group_c, data = mydata_anova1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.86090264 -0.87620916 0.11470254 0.69625303 1.90242723 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.00000000 0.22360680 0.00000 1.0000000 ## group_b 1.00000000 0.31622777 3.16228 0.0025088 ** ## group_c 0.50000000 0.31622777 1.58114 0.1193800 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1 on 57 degrees of freedom ## Multiple R-squared: 0.14925373, Adjusted R-squared: 0.11940299 ## F-statistic: 5 on 2 and 57 DF, p-value: 0.0099839296 7.2 Two-way ANOVA 7.3 ANCOVA For example, if there were only two groups, we would be carrying out one comparison: the mean of Group A vs the mean of Group B. At a 0.05 level of significance, there would be a 5% chance of a Type I error. If we had three groups, there would be three comparisons (Group A vs Group B, Group A vs Group C, and Group B vs Group C), and we would have a 14.3% (1-0.95^3) chance of a Type I error.↩ "]
]
