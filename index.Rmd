--- 
title: "Statistical tests as linear models"
author: "Steve Doogue"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
description: This is a minimal example of using the bookdown package to write a book.
  The output format for this example is bookdown::gitbook.
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
---

```{r, echo = FALSE}
library(knitr)
```

# Introduction

This is a work through of the book [Common statistical tests are linear models (or: how to teach stats)](https://lindeloev.github.io/tests-as-linear/) by Jonas Kristoffer Lindelov. The book demonstrates how most common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximations.

> This beautiful simplicity means that there is less to learn. In particular, it all comes down to $y = a \cdot x + b$ which most students know from highschool. Unfortunately, stats intro courses are usually taught as if each test is an independent tool, needlessly making life more complicated for students and teachers alike.


This aims to solidy my understanding of the original book by replicating the examples providing, and by adding additional background information to fill gaps in my knowledge.

# The data {#data}

## Sample values {#samplevalues}
Most of the examples in the book are based on three imaginary samples (`x`, `y` and `y2`). Each is normally distributed and is made up of 50 observations:

* `x` has a mean of 0 and a standard deviation of 1
* `y` has a mean of 0.3 and a standard deviation of 2
* `y2`has a mean of 0.5 and a standard deviation of 1.5

`x` and `y` can be thought of as samples from different populations (e.g. 50 women and 50 men), while `y` and `y2` are samples of the same individuals at different points in time (the same 50 men before and after participating in a new drug trial).

We begin by creating a function that will allow us to produce samples of a given size (`N`) with a specified mean (`mu`) and standard deviation (`sd`):

```{r, message = FALSE}
library(tidyverse)
rnorm_fixed = function(N, mu = 0, sd = 1){scale(rnorm(N))*sd + mu}
```

Now we can create our three samples:

```{r}
# Set the seed so our 'random' results match those in the original book
set.seed(40)

# Create the samples:
x = rnorm_fixed(N = 50, mu=0, sd=1)
y = rnorm_fixed(N = 50, mu=0.3, sd=2)
y2 = rnorm_fixed(N = 50, mu=0.5, sd=1.5)
```

Lets also combine these samples into 'wide' and 'long' data frames. This doesn't change any of the values, it just rearranges to data into different layouts which can sometimes be easier to work with (e.g. when producing plots):
```{r, warning = FALSE}
# The wide layout is a dataframe with three columns, one for each of x, y and y2
mydata_wide <- tibble(x = x, y = y, y2 = y2)

# the long layout has two columns: one listing the group to which each observation 
# belongs (x, y or y2), and another column with the corresponding value
mydata_long <- mydata_wide %>% 
  gather(group,value,x:y2)
  
```

Here's what our three samples look like when plotted. Note the different mean (0.0, 0.3 and 0.5) and the different 'spread' of values for each group (reflecting their different standard deviations).

```{r, echo = FALSE, warning = FALSE, fig.cap = 'Our sample data',fig.align = 'center',fig.width= 5,fig.height=4}
ggplot(mydata_long, aes(x=group, y = value, color = group)) +
  geom_jitter(width = 0.1, alpha = .75, size =2) + 
  stat_summary(fun.y= mean, 
                  fun.ymin=mean, 
                  fun.ymax=mean, 
                  geom="crossbar", 
                  width=0.5, 
                  size=0.2,
                  color="black") +
  stat_summary(fun.y=mean, 
                  colour="black", 
                  geom="text",
                  show_guide = FALSE, 
                  vjust=-0.7, 
                  aes(label=round(..y..,2))) 
```

## Rank-transformed values {#rank_trans}

Most common tests demonstrated in the book use the _actual_ values from the samples above. However, some tests also use their _rank-transformed_ values. Specifically, the _signed rank_ of the values is used. 

What is meant by signed ranks? The signed rank is found by:  
1. taking the _absolute_ value of each observation in the original sample; that is, expressing all the values as a positive number;  
2. finding the rank of each of these absolute values, where the smallest absolute value has a rank of 1; and  
3. giving each of these ranks the same sign (+ or -) as the original value.

For example, the numbers -2, 3, 7, -25, -30, 31 would have the signed ranks of -1, 2, 3, -4, -5, 6.

We create a function, `signed_rank()`, that we can use later to convert our actual values into signed ranks:

```{r}
# Takes any list of values, x, and calculates their signed rank
signed_rank = function(x) {sign(x) * rank(abs(x))}
```

Here is what our sample data would look like if we used signed ranks instead of actual values (I have remove the horizonal 'jitter' from this chart). Because each sample has 50 observations, each signed rank is somewhere between -50 and +50:

```{r, echo = FALSE, warning = FALSE, fig.cap = 'Our sample data, with signed ranks',fig.align = 'center',fig.width= 5,fig.height=4}
mydata_long_signed <- mydata_wide %>% 
  mutate_all(signed_rank) %>% 
  gather(group,signed_rank,x:y2)

ggplot(mydata_long_signed, aes(x=group, y = signed_rank, color = group)) +
  geom_jitter(width = 0, alpha = .75, size = 2) + 
  stat_summary(fun.y= mean, 
                  fun.ymin=mean, 
                  fun.ymax=mean, 
                  geom="crossbar", 
                  width=0.5, 
                  size=0.2,
                  color="black") +
  stat_summary(fun.y=mean, 
                  colour="black", 
                  geom="text",
                  show_guide = FALSE, 
                  vjust=-0.7, 
                  aes(label=round(..y..,1))) 
```


# The linear model 

## Overview of the linear model

Many common statistical tests are really just special cases of the linear model, or at least a close approximation. 

What is meant by linear model? This section provides an overview of the linear model. The description may be somewhat fast and loose, as it aims to provide an intuitive explanation rather than a rigorous technical description. 

The linear model, or linear regression model, estimates the relationship between one variable and one or more other variables. It is assumed that the relationship can be described as a straight line.  

For example, assume we are looking at a variable $y$ and we want to know its relationship with a variable $x$. We assume that the relationship can be expressed as a mathematical relationship between $y$ (the dependent variable) and $x$ (the explanatory variable):

$y = \beta_0 + \beta_1 x$

To illustrate what this equation is showing, imagine we have six observations of variables $x$ and $y$, which can be plotted as follows:

```{r, echo = FALSE, fig.cap = 'Six observations of x and y', out.width = '60%',fig.align = 'center'}
knitr::include_graphics("images/image01.png")
```

We assume that this relationship can be represented by a straight line. The line is composed of an intercept ($\beta_0$) and a slope ($\beta_1$). Each point on this line represents our _predicted_ value of $y$ for a given value of $x$.

```{r, echo = FALSE, fig.cap = 'Intercept and slope of model', out.width = '60%',fig.align = 'center'}
knitr::include_graphics("images/image02.png")
```

So how do we estimate the intercept and slope? In other words, how do we estimate what the values of $\beta_0$ and $\beta_1$ should be?  

Linear regression estimates the line (i.e., slope and intercept) that minimizes the difference between our _predicted_ values of $y$ and the _actual_ values of $y$ that were observed in the original sample. These difference are referred to as "residuals". 

More specifically, linear regression minimizes the sum of the _squared_ value of these residuals. So in the figure below, linear regression is used to estimate the line that would minimise the combined area of the purple squares. For this reason, the method is sometimes referred to as an "ordinary least squares" (OLS) regression.

```{r, echo = FALSE, fig.cap = 'Line of best fit minimizes the sum of squared residuals', out.width = '60%',fig.align = 'center'}
knitr::include_graphics("images/image03.png")
```

In the example above, we considered a dependent variable ($y$) that was being "explained" by one other variable, or predictor ($x$). But this can be expanded to include multiple predictors, for example with the model:

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 ... + \beta_i x_i$

This is referred to as _multiple regression_. In this case we still have only one intercept ($\beta_0$) but many slopes that need to be estimated ($\beta_1$, $\beta_2$ and so on). The same method can be used to estimate these parameters, i.e. an OLS regression which minimizes the sum of squared residuals. This cannot be illustrated in a two-dimensional diagram, as was the case when a single predictor was used, but the exact same concept applies. 

So that's the linear model. The key premise of Lindelov's book many statistical tests are just special cases of this:  

> Everything below, from one-sample t-test to two-way ANOVA are just special cases of this system. Nothing more, nothing less.

## Estimating linear models in R

To estimate linear models in R we use the `lm()` function.

For example, say we want to estimate the relationship between our sampled data for `x` and `y`. We will apply the following model: $y = \beta_0 + \beta_1 x$

In R, this can be written as follows:
```{r, eval = FALSE}
lm(y ~ 1 + x)  # Represents y = beta0 + beta1 * x
```

In the original book, the specified model is accompanied by the null hypothesis, $H_0: \beta_1=0$. This is equivalent to saying that there is no relationship between `x` and `y`. The output from our linear model tells us whether there could be grounds for rejecting this null hypothesis of "no relationship", in favor of the alternative hypothesis that there _is_ a relationship. 

A key output of interest, or test statistic, is the p-value. 

* A small p-value (conventionally ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis of no relationship.
* A larger p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis of no relationship.

Lets see the output of the lineary model for our sample data:
```{r}
lm(y ~ 1 + x) %>% summary() %>% print(digits = 5) 
```

As seen in the output above, $\beta_1$ (the coefficient on `x`) has a p-value of 0.1053. This means we would fail to reject the null hypothesis that there was no relationship between `x` and `y`. 

## Assumptions

The linear model described above is only valid if a number of assumption hold. 



# Correlations

Correlation is a measure of the strength and direction of association that exists between two variables. Correlation coefficents ($r$) assume values in the range from −1 to +1, where ±1 indicates the strongest possible positive or negative correlation and 0 indicates no linear association between the variables. 

## Pearson correlation

We start by looking at the Pearson correlation coefficient. The following built-in R function can be used to assess the correlation between variables `x` and `y`. 

```{r}
cor.test(y, x, method = "pearson") # Built-in test
```
The output shows that the correlation coefficient ($r$) is -0.2318, with a p-value of 0.1053. On this basis, we would not reject the null hypothesis that the true correlation is equal to 0. 

Now we can demonstrate that the linear model gives the exact same t-value and p-value as the Pearson correlation. The equivalent linear model is:

$y = \beta_0 + \beta_1x  \qquad  H_0: \beta_1 = 0$

This model is run in R using the following commands. From the output, we see that the cofficient on `x`, $\beta_1$, has a p-value of 0.1053, which is the same as the p-value above.

```{r}
lm(y ~ 1 + x) %>% 
  summary() %>% print(digits = 5)
```

The difference is that the linear model returns the _slope_ of the relationship, $\beta_1$ (which in this case is -0.4636), rather than the correlation coefficient, $r$. The slope is usually much more interpretable and informative than the correlation coefficient. However it is useful to understand how the two are related, which is by the following formula:

$\beta_1 = r * sd_y / sd_x$

This shows that:

* When both `x` and `y` have the same standard deviations ($sd_x$ and $sd_y$) then the slope ($\beta_1$) will be equal the correlation coefficient ($r$)
* The ratio of the slope to the correlation coefficient ($\beta_1 / r$) is equal to the ratio of the standard deviations ($sd_y / sd_x$). We know that when we set up our data in Section \@ref(samplevalues) that the standard deviation of `y` was twice as large as `x`. This is why the value of the slope (-0.4636) is twice the size of the correlation coefficient (-0.2318),
* The slope from the linear model will always have the same sign (+ or -) as the correlation coefficient (as standard devaiations are always positive).



## Spearman correlation

There will be times when it is more appropriate to use the __Spearman rank correlation__ than the Pearson correlation.

1.  When the relationship between the variables is not linear, i.e. not a straight line;
2.  When your data is not normally distributed;^[Technically the variables should have _bivariate_ normality, which means they are normally distrbuted when added together, but this is complex and so it is common just to assess whether the variables are individually normal [(source)](https://statistics.laerd.com/spss-tutorials/pearsons-product-moment-correlation-using-spss-statistics.php). If bivariate normality does not hold then you will still get a fair estimate of $r$, but the inferential tests (t-statistics and p-values) could be misleading [(source)](https://www.researchgate.net/post/Why_should_data_be_normally_distributed_and_continuous_in_order_to_apply_Pearson_correlation).]
3.  When you data has large outliers; or
4.  When you are working with ordinal rather than continuous data.^[see Chapter \@ref(appendixtypes) for a description of the different types of data.]

The Spearman correlation is a _non-parameteric_ test as it does not require that the parameters of the linear model hold true - for example, there does not need to be a linear relationship between the two variables, and the data does not need to be normally distributed. 

The Spearman rank correlation is the same as a Pearson correlation but using the _rank_ of the values in our samples.This is an approximation only, 

in this case, the rank of values in `x` and `y`. 

```{r, eval=FALSE}
# Spearman
cor.test(y, x, method = "spearman")

# Pearson using ranks
cor.test(rank(y), rank(x), method = "pearson")

# Linear model using rank
lm(rank(y) ~ 1 + rank(x)) %>% summary() %>% print(digits = 5)
```



```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test,~'correlation',~slope,~p.value,
    'cor.test (Spearman)',-0.2266,'',0.1135,
    'cor.test (Pearson with ranks)',-0.2266,'',0.1135,
    'lm (with ranks)','',-0.2266,0.1135
    ),
  caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

Note that we can interpret the slope which is the number of ranks $y$ changes for each rank on $x$. 

# Appendix - Types of variables {#appendixtypes}

Variables (or measurements) fall into two categories: __discrete__ or __continuous__.

## Discrete variables

Discrete variables are also known as categorical variables. They are descriptions of categories into which observations can fall. Discrete variables can be further categorized as either _nominal_ or _ordinal_.

__Nominals__ variables have two or more categories which do not have an intrinsic order. In other words, there is no basis for ranking the categories. Examples of nominal variables include:

* Whether a person has a landline telephone could be categorized as "yes" or "no" (two categories)
* The US state in which a person lives (50 categories)

__Ordinal__ variables have two or more catogories which do have an intrinsic order - that is, they _can_ be ordered or ranked. Examples include:  

* Levels of agreement, e.g. asking a survey respondent if they (i) strongly agree, (ii) agree, (iii) neither agree nor disagree, (iv) disagree or (v) strongly disagree with a question.
* Educational attainment could be recorded in a survey using four categories: (i) no high school degree, (ii) high school degree, (iii) college degree, or (iv) postgraduate degree. Here the categories can be ranked based on the level of educational attainment. 

For ordinal variables, the interval or distance between the categories does not have a meaningful interpretation. For example, we cannot say that the distance between (i) no highschool degree and (ii) high school degree is the same as the distance bewteen (iii) college degree and (iv) postgraduate degree. 

## Continuous variables

Continuous variables are numbers rather than categories. Continuous variables can be further categorized as either _interval_ or _ratio_ variables.

__Interval__ variables have a numeric value and can be measured along a continuum. The difference between values is interpretable. An example is temperature measure in Fahrenheit: the difference between 20F and 30F is the same as the difference bewtween 30F to 40F. However Fahrenheit is not a ratio variables. For example, 40 degrees is not "twice as hot" as 20 degrees. 

__Ratio__ variables are interval variables for which you can construct a meaningful fraction. Examples include height, weight, distance and income. For example, you could say that an income of \$40,000 was twice as much as an income of \$20,000. "Count" variables are also ratio variables; for example, the number of survey respondents who would vote for a presidential candidate. A condition of ratio variables is that 0 (zero) of the measurement indicates that there is none of that variable (e.g. $0 indicates zero income). This was not the case of temperature measured in Farenheit, as 0F does not mean there is "no temperature". 

The four types of variables above form a hierarchy, where ratio variables are the highest:  

Nominal < Ordinal < Interval < Ratio

At each level up the hierarchy, the current level includes all of the qualities of the one below it and adds something new.