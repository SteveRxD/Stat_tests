# One mean

This set of tests deals with a single mean. They tell us whether, based on our sample, we have reason to believe that the mean of the underlying population differs from a specified level (the 'null hypothesis').

In some cases we might have two samples with _paired_ observations (e.g. an athlete's average running speed before and after a new training technique is introduced). In this case we take the _difference_ between the paired observations (the change in each athlete's running speed) and treat this as a single measurement. We then assess whether, based on the mean of these differences in our sampple, we have reason to believe that the 'true' mean differs from a specified level (e.g. zero).

The _one sample_ and _paired sample_ cases are addressed in turn, below.

## One sample

### One-sample t-test

A one-sample t-test (or one-sample Student's t-test) is used to determine whether a sample could have come from a population with a specific mean. This population mean is not always known and may only be theoretical / hypothesized. 

For example, say we sample pupils who have received a new teaching method. These pupils receive an average score of 70% in an end-of-year test, while the average score for all pupils nationally is 60%. We want to know how often we are likely to see a sample average of 70% (or more extreme) if the 'true' average for pupils receiving the new teaching method was 60%, i.e. was no different from the national average.

__Student's t-test:__ 

In R, we can assess this using a one-sample Student's [t.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/t.test). 


__Equivalent linear model:__

The one-sample t-test is the same as a linear model containing a dependent variable, $y$, and a constant:
$y = \beta_0  \qquad  H_0: \beta_0 = 0$

This is the same as the equation $y = \beta_0 + \beta_1 \cdot x$ that was shown earlier (Chapter \@ref(linearmodel)), but having dropped the last term. This is shown graphically below. Note that this is the same as our original regression, but treating all values of $x$ as zero:

```{r, echo = FALSE, fig.cap = 'Linear model equivalent to one-sample t-test', out.width = '40%', fig.align = 'center'}
knitr::include_graphics("images/image04.png")
```


The estimated intercept ($\beta_0$) is simply the average of all the values in our sample. The linear regression returns the t-statistic and p-value based on the null hypothesis that the average (or $\beta_0$) is equal to zero.

__Comparison:__ 

Here is how you would run the t-test and equivalent linear model in R:

```{r, eval=FALSE}
# t-test (built-in test)
t.test(y, mu = 0, alternative = 'two.sided') 

# Linear model
lm <- lm(y ~ 1)
  lm %>% summary() %>% print(digits = 5) # show summary output
  confint(lm) # show confidence intervals
```

The output of this code, including the 95 percent confidence intervals, is shown in the table below. The outputs are exactly the same!

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test,~'mean / intercept',~t,~p.value,~conf.low,~conf.high,
    't.test',0.3,1.0607,0.294,-0.-0.2684,0.8684,
    'lm',0.3,1.0607,0.294,-0.2684,0.8684
    ),
  caption = 'One-sample t-test and linear model',
  booktabs = TRUE
)
```

##### A null hypothesis other than zero: {-}
By default, the t-test has a null hypothesis is that the sample comes from a population with a mean of _zero_. This was written as $H_0: \beta_0 = 0$ above. What if we want to use a different null hypothesis? For example, to how likely it is that our sample `y` came from a population with a mean of 0.1?

This can be done by specifying the null hypothesis in the built-in t-test (in this case using the argument ` mu = 0.1`) or, in the linear model, by first subtracting this amount from the dependent variable (using `y - 0.1` as the dependent variable).

The code is as follows:

```{r, eval=FALSE}
# t-test (built-in test)
t.test(y, mu = 0.1, alternative = 'two.sided') 

# Linear model
lm((y - 0.1) ~ 1) %>% 
  summary() %>% print(digits = 5) # show summary output
```
The two tests above give identical t statistics and p-values.

### Wilcoxen signed-rank

What if we cannot assume that our population is normally distributed? Here we need to use a non-parametric version of the t-test. 

__Wilcoxen signed-rank test:__

In this case we could use the Wilcoxon signed-rank test, which is the non-parametric version of the t-test. Specifically, we use the one-sample Wilcoxon test.

The Wilcoxon signed-rank test is very similar to the linear model shown above, just with the _signed ranks_ of $y$ instead of $y$ itself. The concept of signed ranks was explained in section \@ref(ranktrans).

It is implemented in R using the [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test), as shown below.

__Equivalent linear model:__

In this case the equivalent linear model is:  

$signed\_rank(y) = \beta_0  \qquad  H_0: \beta_0 = 0$

[Lindelov shows](https://lindeloev.github.io/tests-as-linear/simulations/simulate_wilcoxon.html) that the linear model will be a good approximation of the Wilcoxon signed-rank test when the sample size is larger than 14 and almost perfect when the sample size is larger than 50.

__Comparison:__

The code below is for the [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test) in R and the equivalent linear model using signed ranks:

```{r, eval=FALSE}
# Wilcox test (built-in)
wilcox.test(y)

# Equivalent linear model
lm <- lm(signed_rank(y) ~ 1)  
  lm %>% summary() %>% print(digits = 8) # show summary output
```
```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test,~'mean / intercept',~p.value,
    'wilcox.test','',0.2628,
    'lm (signed ranks)',4.66,0.2650
    ),
  caption = 'Wilcox test and linear model',
  booktabs = TRUE
)
```

The two tests give similar (though not quite identical) p-values. 


## Paired samples

### Paired-sample t-test

A __paired-sample__ t-test (sometime called a dependent-sample t-test) is used to compare two population means where you have two samples in which observations in one sample can be paired with observations in the other sample. 

Common applications of the paired-sample t-test include case-control studies or repeated-measures designs. Examples of when you might use a paired-sample t-test:  

* Before-and-after observations on the same subjects (e.g. studentsâ€™ test
results before and after taking a course).  
* A comparison of two different treatments, where where the treatments are applied to the same subjects (e.g. athletes' ability to lift weights following two different warm-up routines).
* A comparison of two different measurements, where where the measurements are applied to the same subjects (e.g. blood pressure measured using two types of machines).

__Paired-sample t-test / dependent-sample t-test:__

The paired-sample t-test determines whether the average difference between two sets of observations is zero. To run this in R, we use the same built-in Student's t-test as above, but now including both variables as arguments and specifying that we have paired observations (the argument `paired = TRUE`).

__Equivalent linear model:__

The equivalent linear model is the difference between the two observations regressed against a constant:
$y_2 - y_1 = \beta_0 \qquad  H_0: \beta_0 = 0$

__Comparison:__

We can compare the t-test and linear model using our sample data (section \@ref(samplevalues)) as an example. Recall that `y` had a mean of 0.3, and `y2` had a mean of 0.5.

The code and the outputs are shown below. Again, the linear model gives exactly the same results as the t-test!

```{r, eval=FALSE}
# t-test (built-in test)
t.test(y2,y, paired = TRUE, mu = 0, alternative = 'two.sided')

# Equivalent linear model
lm <- lm(y2 - y ~ 1)
  lm %>% summary() %>% print(digits = 8)
  confint(lm)
```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test,~'mean / intercept',~t,~p.value,~conf.low,~conf.high,
    't.test', 0.2, 0.5426, 0.5898, -0.5407, 0.9407,
    'lm', 0.2, 0.5426, 0.5898, -0.5407, 0.9407
    ),
  caption = 'Paired-sample t-test and linear model',
  booktabs = TRUE
)
```


### Wilcoxen matched pairs

If the necessary assumptions do not hold for a paired-sample t-test, we can use the non-parameteric counterpart. This is the __Wilcoxen matched pairs__. 

For this we use the built-in [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test) in R, including both variables and specifying that we have paired observations (the argument `paired = TRUE`).

The equivalent linear model is exactly the same above but using _signed rank_ of the difference between observations, i.e.:

$signed\_rank(y_2 - y_1) = \beta_0 \qquad  H_0: \beta_0 = 0$

A comparison of the outputs is shown below. The p-values are almost identical. The t-test has also been included, this time using signed ranks, to show that this is the same as the linear model. 

```{r, eval=FALSE}
#  Wilcox test (built-in test)
wilcox.test(y2,y, paired = TRUE, mu = 0, alternative = 'two.sided')

# Equivalent linear model
lm <- lm(signed_rank(y2 - y) ~ 1)
  lm %>% summary() %>% print(digits = 8)
  
# t-test using signed ranks (built-in test)
t.test(signed_rank(y2-y), mu = 0, alternative = 'two.sided')
```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test,~'mean / intercept',~p.value,
    'wilcox.test','',0.7795,
    'lm (signed ranks)', 1.18,0.7790,
    't.test (signed ranks)', 1.18, 0.7790 
    ),
  caption = 'Wilcox test and linear model, with paired samples',
  booktabs = TRUE
)
```